{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chubby-natural",
   "metadata": {},
   "source": [
    "## This is my crawler for Amazon.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-brown",
   "metadata": {},
   "source": [
    "### The crawler will built in python using;\n",
    "- Functional Programming\n",
    "- Object Oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-prerequisite",
   "metadata": {},
   "source": [
    "### First we import libraries for the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sized-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.common.by import By     \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-characteristic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "painful-riverside",
   "metadata": {},
   "source": [
    "### Then we create our header variable, This is for sites that return something different from what is supposed to be there, maybe as a result of client side languages or it was deliberate to ward off harmfull web crawlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "future-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ({'User-Agent':\n",
    "        'Mozilla/5.0 (X11; Linux x86_64)\\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko)\\\n",
    "        Chrome/44.0.2403.157 Safari/537.36',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ordered-registration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "unique-slide",
   "metadata": {},
   "source": [
    "## Scraping the site using functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "critical-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amazon_product_details(url):\n",
    "    amazon = {}\n",
    "    driver = webdriver.Chrome(executable_path=r'C:\\Users\\Jchukwuedozi\\Downloads\\chromedriver_win32\\chromedriver.exe')\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    page_source = driver.page_source\n",
    "    soup = bs(page_source)\n",
    "    try:\n",
    "        amazon['Title'] = soup.find(id='productTitle').get_text(' ', strip=True)\n",
    "    except Exception:\n",
    "        amazon['Title'] = None\n",
    "    try:\n",
    "        amazon['Price'] = soup.find(class_='a-color-price').get_text(' ', strip=True)\n",
    "    except Exception:\n",
    "        amazon['Price'] = None\n",
    "    try:\n",
    "        amazon['Review'] = soup.find(class_='a-icon-alt').get_text().split(' ')[0]\n",
    "    except Exception:\n",
    "        amazon['Review'] = None\n",
    "    try:\n",
    "        amazon['Rating'] = soup.find(id='acrCustomerReviewText').get_text().split(' ')[0]\n",
    "    except Exception:\n",
    "        amazon['Rating'] = None\n",
    "    try:\n",
    "        table = soup.find('table', class_='a-spacing-micro').find_all('tr')\n",
    "    except Exception:\n",
    "        print(\"a-spacing-micro can't be found\")\n",
    "    else:\n",
    "        for row in table:\n",
    "            content = row.find_all('td')\n",
    "            key = content[0].get_text(' ', strip=True)\n",
    "            value = content[1].get_text(' ', strip=True)\n",
    "            amazon[key] = value\n",
    "    \n",
    "    try:\n",
    "        product_details = soup.find('div', id='detailBullets_feature_div'\n",
    "                ).find('ul', class_='a-unordered-list')\n",
    "    except Exception:\n",
    "        print(\"detailBullets_feature_div can't be found\")\n",
    "    else:\n",
    "        for category in product_details.find_all('li'):\n",
    "            product_key = [item.get_text(' ', strip=True).split('\\n')[0] for item in category.find_all(class_='a-text-bold')]\n",
    "            product_value = [item.find_next_sibling().get_text() for item in category.find_all(class_='a-text-bold')]\n",
    "            for key, value in zip(product_key, product_value):\n",
    "                amazon[key] = value\n",
    "    return amazon    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "little-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_products(url, page_num):\n",
    "    all_amazon_products = []\n",
    "    driver = webdriver.Chrome(executable_path=r'C:\\Users\\Jchukwuedozi\\Downloads\\chromedriver_win32\\chromedriver.exe')\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        page_source = driver.page_source\n",
    "        soup = bs(page_source)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        pages = int(soup.find_all('li', class_='a-disabled')[-1].get_text())\n",
    "        for page in range(2):\n",
    "            links = [i.get_attribute('href') for i in driver.find_elements_by_css_selector('.a-link-normal.s-no-outline')]\n",
    "            for link in links:\n",
    "                all_amazon_products.append(amazon_product_details(link))\n",
    "            page_num +=1\n",
    "            url = f'https://www.amazon.com/s?i=specialty-aps&rh=n%3A18505451011&fs=true&page={page_num}&qid=1625791103&ref=sr_pg_2'\n",
    "            driver.get(url)\n",
    "            page_source = driver.page_source\n",
    "            soup = bs(page_source)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return all_amazon_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "compliant-january",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping... https://www.amazon.com/AXE-Shower-Tool-Detailer-Pack/dp/B0017TZD7S/ref=sr_1_17?dchild=1&qid=1626044199&sr=8-17\n",
      "a-spacing-micro can't be found\n",
      "scraping... https://www.amazon.com/Strip-Eyelash-Adhesive-Clear-0-176/dp/B01BMMOAFU/ref=sr_1_18?dchild=1&qid=1626044199&sr=8-18\n",
      "scraping... https://www.amazon.com/Colgate-Extra-Clean-Toothbrush-Count/dp/B00CC6XSSQ/ref=sr_1_19?dchild=1&qid=1626044199&sr=8-19\n",
      "scraping... https://www.amazon.com/CHI-Guard-Thermal-Protection-Spray/dp/B002RS6JSA/ref=sr_1_20?dchild=1&qid=1626044199&sr=8-20\n",
      "scraping... https://www.amazon.com/CeraVe-Facial-Moisturizing-Lotion-AM/dp/B00F97FHAW/ref=sr_1_21?dchild=1&qid=1626044199&sr=8-21\n",
      "scraping... https://www.amazon.com/Sensodyne-Pronamel-Whitening-Strengthening-Toothpaste/dp/B0762LYFKP/ref=sr_1_22?dchild=1&qid=1626044199&rdc=1&sr=8-22\n",
      "scraping... https://www.amazon.com/Tweezerman-Stainless-Steel-Slant-Tweezer/dp/B000EMYJ88/ref=sr_1_23?dchild=1&qid=1626044199&sr=8-23\n",
      "Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n",
      "  (Session info: chrome=91.0.4472.124)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products = get_all_products('https://www.amazon.com/s?i=specialty-aps&rh=n%3A18505451011&fs=true&page=2&qid=1625791103&ref=sr_pg_2', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-romania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "configured-seller",
   "metadata": {},
   "source": [
    "## Scraping the site with a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "measured-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class in use ------------------------------------------------------------\n",
    "class Amaz:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.driver = webdriver.Chrome(executable_path=r'C:\\Users\\Jchukwuedozi\\Downloads\\chromedriver_win32\\chromedriver.exe')\n",
    "        \n",
    "    def amazon_product_details(self, url):\n",
    "        amazon = {}\n",
    "        self.driver = webdriver.Chrome(executable_path=r'C:\\Users\\Jchukwuedozi\\Downloads\\chromedriver_win32\\chromedriver.exe')\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        self.page_source = self.driver.page_source\n",
    "        self.soup = bs(self.page_source)\n",
    "        try:\n",
    "            amazon['Title'] = self.soup.find(id='productTitle').get_text(' ', strip=True)\n",
    "        except Exception:\n",
    "            amazon['Title'] = None\n",
    "        try:\n",
    "            amazon['Price'] = self.soup.find(class_='a-color-price').get_text(' ', strip=True)\n",
    "        except Exception:\n",
    "            amazon['Price'] = None\n",
    "        try:\n",
    "            amazon['Review'] = self.soup.find(class_='a-icon-alt').get_text().split(' ')[0]\n",
    "        except Exception:\n",
    "            amazon['Review'] = None\n",
    "        try:\n",
    "            amazon['Rating'] = self.soup.find(id='acrCustomerReviewText').get_text().split(' ')[0]\n",
    "        except Exception:\n",
    "            amazon['Rating'] = None\n",
    "        try:\n",
    "            table = self.soup.find('table', class_='a-spacing-micro').find_all('tr')\n",
    "        except Exception:\n",
    "            print(\"a-spacing-micro can't be found\")\n",
    "        else:\n",
    "            for row in table:\n",
    "                content = row.find_all('td')\n",
    "                key = content[0].get_text(' ', strip=True)\n",
    "                value = content[1].get_text(' ', strip=True)\n",
    "                amazon[key] = value\n",
    "\n",
    "        try:\n",
    "            product_details = self.soup.find('div', id='detailBullets_feature_div'\n",
    "                    ).find('ul', class_='a-unordered-list')\n",
    "        except Exception:\n",
    "            print(\"detailBullets_feature_div can't be found\")\n",
    "        else:\n",
    "            for category in product_details.find_all('li'):\n",
    "                product_key = [item.get_text(' ', strip=True).split('\\n')[0] for item in category.find_all(class_='a-text-bold')]\n",
    "                product_value = [item.find_next_sibling().get_text() for item in category.find_all(class_='a-text-bold')]\n",
    "                for key, value in zip(product_key, product_value):\n",
    "                    amazon[key] = value\n",
    "        return amazon    \n",
    "    \n",
    "    \n",
    "    def get_all_product(self, page_num):\n",
    "        all_amazon_products = []\n",
    "        self.page_num = page_num\n",
    "        self.driver.get(self.url)\n",
    "        self.page_source = self.driver.page_source\n",
    "        self.soup = bs(self.page_source)\n",
    "        pages = int(self.soup.find_all('li', class_='a-disabled')[-1].get_text())\n",
    "        for page in range(pages): \n",
    "            links = [i.get_attribute('href') for i in self.driver.find_elements_by_css_selector('.a-link-normal.s-no-outline')]\n",
    "            for link in links:\n",
    "                all_amazon_products.append(self.amazon_products(link))\n",
    "            self.page_num +=1    \n",
    "            url = f'https://www.amazon.com/s?i=specialty-aps&rh=n%3A18505451011&fs=true&page={self.page_num}&qid=1625791103&ref=sr_pg_2'\n",
    "            self.driver.get(url)\n",
    "            self.page_source = self.driver.page_source\n",
    "            self.soup = bs(self.page_source)\n",
    "        return all_amazon_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-beatles",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-james",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-ratio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-conditioning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
